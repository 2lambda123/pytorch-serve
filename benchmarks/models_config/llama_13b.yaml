---
opt_13b:
    eager_mode:
        benchmark_engine: "ab"
        url: "file:///data/home/hamidnazeri/serve/examples/large_models/llama/model_store/llama.mar"
        workers:
            - 1
        batch_delay: 300000
        batch_size:
            - 1
        requests: 1000
        concurrency: 100
        input: "./examples/large_models/llama/sample_text.txt"
        backend_profiling: False
        exec_env: "local"
        processors:
            - "gpus": "all"