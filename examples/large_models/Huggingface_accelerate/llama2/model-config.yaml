# TorchServe frontend parameters
minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
responseTimeout: 1200
deviceType: "gpu"

handler:
    model_name: "meta-llama/Llama-2-7b-chat-hf"
    model_path: "model/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235"
    max_length: 50
    max_new_tokens: 50
    manual_seed: 40
    fast_kernels: True

micro_batching:
    micro_batch_size: 4
    parallelism:
        preprocess: 2
        inference: 1
        postprocess: 2
