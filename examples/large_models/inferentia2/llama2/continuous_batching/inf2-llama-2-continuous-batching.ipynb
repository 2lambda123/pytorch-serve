{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TorchServe Continuous Batching Serve Llama-2 on Inferentia-2\n",
    "This notebook demonstrates TorchServe continuous batching serving Llama-2-13b on Inferentia-2 `inf2.24xlarge` with DLAMI: Deep Learning AMI Neuron PyTorch 1.13 (Ubuntu 20.04) 20231226"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installation\n",
    "Note: This section can be skipped once [Neuron DLC](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-containers) release TorchServe latest version."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install Python venv\n",
    "!sudo apt-get install -y python3.9-venv g++\n",
    "\n",
    "# Create Python venv\n",
    "!python3.9 -m venv aws_neuron_venv_pytorch\n",
    "\n",
    "# Activate Python venv\n",
    "!source aws_neuron_venv_pytorch/bin/activate\n",
    "!python -m pip install -U pip\n",
    "\n",
    "# Clone Torchserve git repository\n",
    "!git clone https://github.com/pytorch/serve.git\n",
    "\n",
    "# Install dependencies\n",
    "!python ~/serve/ts_scripts/install_dependencies.py --neuronx --environment=dev\n",
    "\n",
    "# Install torchserve and torch-model-archiver\n",
    "python ts_scripts/install_from_src.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create model artifacts\n",
    "\n",
    "Note: run `mv model/models--meta-llama--Llama-2-13b-hf/snapshots/dc1d3b3bfdb69df26f8fc966c16353274b138c55/model.safetensors.index.json model/models--meta-llama--Llama-2-13b-hf/snapshots/dc1d3b3bfdb69df26f8fc966c16353274b138c55/model.safetensors.index.json.bkp`\n",
    " if neuron sdk does not support safetensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# login in Hugginface hub\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN\n",
    "!python ~/serve/examples/large_models/utils/Download_model.py --model_path model --model_name meta-llama/Llama-2-13b-hf --use_auth_token True\n",
    "\n",
    "# Create TorchServe model artifacts\n",
    "!torch-model-archiver --model-name llama-2-13b --version 1.0 --handler inf2_handler.py -r requirements.txt --config-file model-config.yaml --archive-format no-archive\n",
    "!mv model llama-2-13b\n",
    "!mkdir -p ~/serve/model_store\n",
    "!mv ~/serve/llama-2-13b /home/model-server/model_store\n",
    "\n",
    "# Precompile complete once the log \"Model llama-2-13b loaded successfully\"\n",
    "torchserve --ncs --start --model-store /home/model-server/model_store --models llama-2-13b --ts-config ../config.properties"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run single inference request\n",
    "!python ~/serve/examples/large_models/utils/test_llm_streaming_response.py -m llama-2-13b -o 50 -t 2 -n 4 --prompt-text \"Today the weather is really nice and I am planning on \" --prompt-randomize"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
